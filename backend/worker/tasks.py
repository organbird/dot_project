"""
Celery ë¹„ë™ê¸° ì‘ì—… ì •ì˜ ëª¨ë“ˆ

ì£¼ìš” ì‘ì—…:
    - save_chat_task: ì±„íŒ… ë©”ì‹œì§€ë¥¼ MySQLê³¼ Redisì— ì €ì¥
    - ingest_pdf_task: PDF íŒŒì¼ì„ ë²¡í„° DBì— í•™ìŠµ
    - generate_image_task: ComfyUIë¡œ ì´ë¯¸ì§€ ìƒì„±
    - transcribe_audio_task: Faster Whisper STT ë³€í™˜
"""

from worker.celery_app import celery_app
from app.database import SessionLocal
from app import models
import json
import redis
import os
import time
import tempfile
import requests as http_requests
from dotenv import load_dotenv
from worker.gpu_manager import try_acquire, after_task, release_if_idle, GPU_RETRY_COUNTDOWN

load_dotenv()

# ì›Œì»¤ ì „ìš© Redis í´ë¼ì´ì–¸íŠ¸
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
redis_client = redis.from_url(REDIS_URL, decode_responses=True)

# PC1 ë°±ì—”ë“œ API URL
MASTER_API_URL = os.getenv("MASTER_API_URL", "http://backend:8000")

# ì„ë² ë”© ëª¨ë¸ (ì§€ì—° ì´ˆê¸°í™”)
_embedding_model = None

def get_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _embedding_model
    if _embedding_model is None:
        from langchain_huggingface import HuggingFaceEmbeddings
        print("ğŸ“¥ [Worker] ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘... (GPU ëª¨ë“œ)")
        _embedding_model = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sbert-nli",
            model_kwargs={'device': 'cuda'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("âœ… [Worker] ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    return _embedding_model

# ì´ë¯¸ì§€ ìƒì„± ì—”ì§„ (ì§€ì—° ì´ˆê¸°í™”)
_image_engine = None

def _get_image_engine():
    """ì´ë¯¸ì§€ ì—”ì§„ ì‹±ê¸€í†¤ ë°˜í™˜ (ComfyUI API Client)"""
    global _image_engine
    if _image_engine is None:
        from ai_core.image_engine import ImageEngine
        _image_engine = ImageEngine()
        print("ğŸ¨ [Worker] ImageEngine ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ComfyUI API Client)")
    return _image_engine


# =====================================================================
# ê³µí†µ í—¬í¼ í•¨ìˆ˜
# =====================================================================

def _update_task_progress(task_type: str, task_id: str, progress: int, message: str, status: str = "processing"):
    """ì‘ì—… ì§„í–‰ë¥ ì„ Redisì— ì €ì¥ (RAG/ì´ë¯¸ì§€/STT ê³µìš©)"""
    try:
        redis_key = f"{task_type}_task:{task_id}:progress"
        progress_data = {"status": status, "progress": progress, "message": message}
        redis_client.setex(redis_key, 600, json.dumps(progress_data, ensure_ascii=False))
        print(f"ğŸ“Š [{task_type.upper()} Progress] {progress}% - {message}")
    except Exception as e:
        print(f"âš ï¸ [{task_type.upper()} Progress] Redis ì €ì¥ ì‹¤íŒ¨: {e}")


def _call_llm_summary(prompt: str, label: str = "ìš”ì•½") -> str:
    """PC1 LLM APIë¥¼ í˜¸ì¶œí•˜ì—¬ ìš”ì•½ ìƒì„± (ë¬¸ì„œ/íšŒì˜ ê³µìš©)"""
    try:
        backend_url = MASTER_API_URL
        print(f"ğŸ“¡ [Worker] {label}ì„ ìœ„í•´ LLM API í˜¸ì¶œ ì¤‘... ({backend_url})")

        response = http_requests.post(
            f"{backend_url}/ai/chat/generate",
            json={"message": prompt},
            timeout=10
        )

        if response.status_code != 200:
            print(f"âš ï¸ [Worker] LLM API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
            return None

        llm_task_id = response.json().get("task_id")
        print(f"ğŸ“¥ [Worker] LLM {label} ì‘ì—… ì‹œì‘ë¨ (Task ID: {llm_task_id})")

        # Pollingìœ¼ë¡œ ê²°ê³¼ ëŒ€ê¸° (ìµœëŒ€ 120ì´ˆ)
        for attempt in range(120):
            time.sleep(1)
            result_response = http_requests.get(
                f"{backend_url}/ai/tasks/{llm_task_id}",
                timeout=5
            )
            if result_response.status_code == 200:
                result_data = result_response.json()
                if result_data.get("status") == "completed":
                    summary = result_data.get("result", "").strip()
                    print(f"âœ… [Worker] LLM {label} ì‘ë‹µ ë°›ìŒ (ì‹œë„: {attempt + 1}íšŒ)")
                    return summary
                elif result_data.get("status") == "failed":
                    print(f"âš ï¸ [Worker] LLM {label} ìƒì„± ì‹¤íŒ¨: {result_data.get('error')}")
                    return None

        print(f"â±ï¸ [Worker] LLM {label} ì‘ë‹µ íƒ€ì„ì•„ì›ƒ (120ì´ˆ ì´ˆê³¼)")
        return None

    except http_requests.exceptions.Timeout:
        print(f"â±ï¸ [Worker] LLM API ìš”ì²­ íƒ€ì„ì•„ì›ƒ")
        return None
    except http_requests.exceptions.ConnectionError:
        print(f"ğŸ”Œ [Worker] LLM API ì—°ê²° ì‹¤íŒ¨")
        return None
    except Exception as e:
        print(f"âš ï¸ [Worker] {label} ìƒì„± ì¤‘ ì—ëŸ¬: {e}")
        return None


def _generate_document_summary(texts: list) -> str:
    """ë¬¸ì„œ í…ìŠ¤íŠ¸ ì²­í¬ë¡œë¶€í„° LLM ìš”ì•½ ìƒì„±"""
    combined_text = ""
    for chunk in texts:
        if len(combined_text) + len(chunk) > 3000:
            remaining = 3000 - len(combined_text)
            if remaining > 0:
                combined_text += chunk[:remaining]
            break
        combined_text += chunk + "\n"

    if not combined_text.strip():
        return None

    prompt = f"""ë‹¤ìŒì€ PDF ë¬¸ì„œì˜ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
ìš”ì•½ì€ 3~5ë¬¸ì¥, 300ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
ë¬¸ì„œì˜ ì£¼ì œ, í•µì‹¬ ë‚´ìš©, ì£¼ìš” ê²°ë¡ ì„ í¬í•¨í•´ì£¼ì„¸ìš”.

[ë¬¸ì„œ ë‚´ìš©]
{combined_text}

[ìš”ì•½]"""
    return _call_llm_summary(prompt, "ë¬¸ì„œ ìš”ì•½")


def _generate_meeting_summary(transcript: str) -> str:
    """íšŒì˜ ì „ë¬¸ í…ìŠ¤íŠ¸ë¡œë¶€í„° LLM ìš”ì•½ ìƒì„±"""
    text = transcript[:3000] if len(transcript) > 3000 else transcript
    if not text.strip():
        return None

    prompt = f"""ë‹¤ìŒì€ íšŒì˜ ë…¹ìŒì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ íšŒì˜ì˜ í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
ìš”ì•½ì€ 3~5ë¬¸ì¥, 300ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
íšŒì˜ì˜ ì£¼ì œ, ë…¼ì˜ ë‚´ìš©, ì£¼ìš” ê²°ë¡ ì´ë‚˜ ê²°ì •ì‚¬í•­ì„ í¬í•¨í•´ì£¼ì„¸ìš”.

[íšŒì˜ ë‚´ìš©]
{text}

[ìš”ì•½]"""
    return _call_llm_summary(prompt, "íšŒì˜ ìš”ì•½")


# =====================================================================
# ì±„íŒ… ì €ì¥ Task
# =====================================================================

@celery_app.task(name="save_chat_task")
def save_chat_task(session_id: int, user_msg: str, ai_msg: str, ref_docs_json: str):
    """ì±„íŒ… ë©”ì‹œì§€ë¥¼ MySQLê³¼ Redisì— ë¹„ë™ê¸°ë¡œ ì €ì¥"""
    print(f"ğŸ’¾ [Worker] ëŒ€í™” ì €ì¥ ì‹œì‘ (Session: {session_id})")

    db = SessionLocal()
    try:
        # 1. MySQL ì €ì¥
        user_chat = models.ChatMessage(
            session_id=session_id, sender="user",
            content=user_msg, reference_docs=None
        )
        db.add(user_chat)

        parsed_ref_docs = None
        if ref_docs_json:
            try:
                parsed_ref_docs = json.loads(ref_docs_json) if isinstance(ref_docs_json, str) else ref_docs_json
            except json.JSONDecodeError:
                parsed_ref_docs = None

        ai_chat = models.ChatMessage(
            session_id=session_id, sender="assistant",
            content=ai_msg, reference_docs=parsed_ref_docs
        )
        db.add(ai_chat)
        db.commit()
        print("âœ… [Worker] MySQL ì €ì¥ ì™„ë£Œ")

        # 2. Redis ìºì‹œ ê°±ì‹ 
        redis_key = f"session:{session_id}:context"
        try:
            cached_context = redis_client.get(redis_key)
            context_data = json.loads(cached_context) if cached_context else {"summary": None, "messages": []}

            context_data["messages"].append({"sender": "user", "content": user_msg})
            context_data["messages"].append({"sender": "assistant", "content": ai_msg})

            current_count = len(context_data["messages"])
            print(f"âœ… [Worker] Redis ìºì‹œ ì—…ë°ì´íŠ¸ ì™„ë£Œ (í˜„ì¬ ë©”ì‹œì§€ ìˆ˜: {current_count}ê°œ)")

            # ë©”ì‹œì§€ê°€ 10ê°œ ì´ìƒì´ë©´ ì¬ìš”ì•½ íŠ¸ë¦¬ê±°
            if current_count >= 10:
                print(f"ğŸ”„ [Worker] Redis ë©”ì‹œì§€ {current_count}ê°œ - ìë™ ì¬ìš”ì•½ íŠ¸ë¦¬ê±°")
                oldest_two = context_data["messages"][:2]

                from worker.tasks import update_summary_task
                update_summary_task.delay(
                    session_id=session_id,
                    current_summary=context_data.get("summary"),
                    oldest_messages=oldest_two
                )

                context_data["messages"] = context_data["messages"][2:]
                print(f"âœ… [Worker] ì¬ìš”ì•½ íŠ¸ë¦¬ê±° + ì˜¤ë˜ëœ 2ê°œ ì œê±° (ë‚¨ì€: {len(context_data['messages'])}ê°œ)")

            redis_client.setex(redis_key, 3600, json.dumps(context_data, ensure_ascii=False))
        except Exception as redis_err:
            print(f"âš ï¸ [Worker] Redis ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ë¬´ì‹œ): {redis_err}")

    except Exception as e:
        print(f"ğŸ”¥ [Worker] ì €ì¥ ì‹¤íŒ¨: {e}")
        db.rollback()
    finally:
        db.close()


@celery_app.task(name="update_summary_task")
def update_summary_task(session_id: int, current_summary: str, oldest_messages: list):
    """ì„¸ì…˜ ìš”ì•½ì„ ì¬ìƒì„±í•˜ëŠ” ë¹„ë™ê¸° ì‘ì—… (PC1 LLM API í˜¸ì¶œ)"""
    print(f"ğŸ”„ [Worker] ì„¸ì…˜ {session_id} ìš”ì•½ ì¬ìƒì„± ì‹œì‘")

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    context = f"[ê¸°ì¡´ ìš”ì•½]\n{current_summary}\n\n" if current_summary else ""
    context += "[ìƒˆë¡œ ì¶”ê°€ëœ ëŒ€í™”]\n"
    for msg in oldest_messages:
        role_name = "ì‚¬ìš©ì" if msg["sender"] == "user" else "AI"
        context += f"{role_name}: {msg['content']}\n"

    prompt = f"""ë‹¤ìŒì€ ì±„íŒ… ì„¸ì…˜ì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤.

{context}

ìœ„ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. í•µì‹¬ ì£¼ì œì™€ ì¤‘ìš”í•œ ì •ë³´ë§Œ í¬í•¨í•˜ì„¸ìš”.
ìš”ì•½ì€ 200ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""

    new_summary = _call_llm_summary(prompt, "ì„¸ì…˜ ìš”ì•½")
    if not new_summary:
        return "ğŸ”¥ ìš”ì•½ ìƒì„± ì‹¤íŒ¨"

    # DB ì—…ë°ì´íŠ¸
    db = SessionLocal()
    try:
        session = db.query(models.ChatSession).filter(models.ChatSession.id == session_id).first()
        if session:
            session.current_summary = new_summary
            db.commit()
            print(f"âœ… [Worker] ì„¸ì…˜ {session_id} MySQL ìš”ì•½ ì—…ë°ì´íŠ¸ ì™„ë£Œ")

            # Redis ë™ê¸°í™”
            try:
                redis_key = f"session:{session_id}:context"
                cached_context = redis_client.get(redis_key)
                if cached_context:
                    context_data = json.loads(cached_context)
                    context_data["summary"] = new_summary
                    redis_client.setex(redis_key, 3600, json.dumps(context_data, ensure_ascii=False))
                    print(f"âœ… [Worker] Redis ìš”ì•½ë„ ë™ê¸°í™” ì™„ë£Œ")
            except Exception as redis_err:
                print(f"âš ï¸ [Worker] Redis ìš”ì•½ ë™ê¸°í™” ì‹¤íŒ¨ (ë¬´ì‹œ): {redis_err}")

            return "âœ… ìš”ì•½ ì—…ë°ì´íŠ¸ ì™„ë£Œ"
        else:
            return f"âš ï¸ ì„¸ì…˜ {session_id} ì—†ìŒ"
    except Exception as e:
        print(f"ğŸ”¥ [Worker] DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        db.rollback()
        return f"ğŸ”¥ DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}"
    finally:
        db.close()


# =====================================================================
# PDF ë²¡í„°í™” Task
# =====================================================================

@celery_app.task(name="ingest_pdf_task", bind=True)
def ingest_pdf_task(self, file_path: str):
    """PDFë¥¼ ë²¡í„°í™”í•˜ì—¬ PC1 ChromaDBì— ì €ì¥"""
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter

    task_id = self.request.id
    print(f"ğŸ“¥ [Worker] PDF í•™ìŠµ ì‹œì‘: {file_path} (Task ID: {task_id})")

    file_name = os.path.basename(file_path)
    chroma_id = file_name.split('.')[0]

    _update_task_progress("rag", task_id, 5, "ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    db = SessionLocal()
    tmp_path = None
    try:
        # 1. PC1ì—ì„œ PDF ë‹¤ìš´ë¡œë“œ
        _update_task_progress("rag", task_id, 10, "PDF íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        download_url = f"{MASTER_API_URL}/document/internal/file/{file_name}"
        resp = http_requests.get(download_url, timeout=60)
        if resp.status_code != 200:
            raise RuntimeError(f"PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {resp.status_code} - {resp.text}")

        with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp:
            tmp.write(resp.content)
            tmp_path = tmp.name

        _update_task_progress("rag", task_id, 20, "PDF ë‹¤ìš´ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

        # 2. PDF íŒŒì‹± + ì²­í¬ ë¶„í• 
        _update_task_progress("rag", task_id, 25, "PDF ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        loader = PyPDFLoader(tmp_path)
        docs = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        splits = text_splitter.split_documents(docs)
        _update_task_progress("rag", task_id, 35, f"í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ ({len(splits)}ê°œ ì²­í¬)")

        # 3. ì„ë² ë”© ìƒì„±
        _update_task_progress("rag", task_id, 40, "ì„ë² ë”© ëª¨ë¸ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        model = get_embedding_model()
        _update_task_progress("rag", task_id, 50, "ë¬¸ì„œ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        texts = [s.page_content for s in splits]
        metadatas = [{"source": file_path, "page": s.metadata.get("page", 0)} for s in splits]
        embeddings = model.embed_documents(texts)

        # 4. PC1ìœ¼ë¡œ ë²¡í„° ì „ì†¡
        _update_task_progress("rag", task_id, 60, "ë²¡í„° ë°ì´í„°ë¥¼ ì„œë²„ë¡œ ì „ì†¡í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        store_url = f"{MASTER_API_URL}/document/internal/store-vectors"
        store_resp = http_requests.post(
            store_url,
            json={"embeddings": embeddings, "texts": texts, "metadatas": metadatas},
            timeout=120
        )
        if store_resp.status_code != 200:
            raise RuntimeError(f"ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {store_resp.status_code} - {store_resp.text}")

        # 5. LLM ë¬¸ì„œ ìš”ì•½
        _update_task_progress("rag", task_id, 70, "AIê°€ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        doc_summary = _generate_document_summary(texts)

        # 6. DB ì—…ë°ì´íŠ¸
        _update_task_progress("rag", task_id, 90, "ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        doc = db.query(models.Document).filter(models.Document.chroma_id == chroma_id).first()
        if doc:
            doc.status = "INDEXED"
            if doc_summary:
                doc.summary = doc_summary
            db.commit()

        result = f"ì €ì¥ ì™„ë£Œ! (ì´ {len(splits)}ê°œì˜ ì¡°ê°ìœ¼ë¡œ ë¶„í• ë¨)"
        _update_task_progress("rag", task_id, 100, "ë¬¸ì„œ ë²¡í„°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!", "completed")
        return result

    except Exception as e:
        error_msg = f"í•™ìŠµ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}"
        print(f"ğŸ”¥ {error_msg}")
        _update_task_progress("rag", task_id, 0, f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}", "failed")

        try:
            doc = db.query(models.Document).filter(models.Document.chroma_id == chroma_id).first()
            if doc:
                doc.status = "ERROR"
                db.commit()
        except Exception as db_err:
            print(f"ğŸ”¥ [Worker] DB ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {db_err}")

        return error_msg
    finally:
        if tmp_path and os.path.exists(tmp_path):
            os.unlink(tmp_path)
        db.close()


# =====================================================================
# ì´ë¯¸ì§€ ìƒì„± Task (ComfyUI)
# =====================================================================

@celery_app.task(name="generate_image_task", bind=True, max_retries=20)
def generate_image_task(self, image_id: str, prompt: str, style: str = "realistic",
                        size: str = "1024x1024", user_id: int = None):
    """ComfyUIë¡œ ì´ë¯¸ì§€ë¥¼ ë¹„ë™ê¸° ìƒì„± í›„ PC1ì— ì „ì†¡"""
    task_id = self.request.id
    print(f"ğŸ¨ [Worker] ì´ë¯¸ì§€ ìƒì„± ì‹œì‘ (Task ID: {task_id})")
    print(f"   - Image ID: {image_id}, Style: {style}, Size: {size}")
    print(f"   - Prompt: {prompt[:50]}...")

    # GPU ìì› íšë“
    if not try_acquire("image"):
        print(f"â³ [Worker] GPU ì‚¬ìš© ì¤‘ - {GPU_RETRY_COUNTDOWN}ì´ˆ í›„ ì¬ì‹œë„")
        raise self.retry(countdown=GPU_RETRY_COUNTDOWN)

    _update_task_progress("image", task_id, 5, "ì´ë¯¸ì§€ ìƒì„± ì¤€ë¹„ ì¤‘...")
    file_name = f"{image_id}.png"

    try:
        # 1. ì´ë¯¸ì§€ ì—”ì§„ ì´ˆê¸°í™”
        _update_task_progress("image", task_id, 10, "ì´ë¯¸ì§€ ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
        engine = _get_image_engine()

        # 2. ComfyUI ì—°ê²° í™•ì¸
        if not engine.is_loaded():
            _update_task_progress("image", task_id, 15, "ComfyUI ì„œë²„ ì—°ê²° ì¤‘...")
            engine.load_model()
            _update_task_progress("image", task_id, 30, "ComfyUI ì—°ê²° ì™„ë£Œ")
        else:
            _update_task_progress("image", task_id, 30, "ComfyUI ì¤€ë¹„ ì™„ë£Œ (ì—°ê²°ë¨)")

        # 3. ì´ë¯¸ì§€ ìƒì„±
        _update_task_progress("image", task_id, 35, "ComfyUIì—ì„œ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
        image_bytes = engine.generate(
            prompt=prompt, style=style, size=size,
            num_inference_steps=28, guidance_scale=4.5,
            progress_callback=None
        )

        _update_task_progress("image", task_id, 87, "ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ, PC1ìœ¼ë¡œ ì „ì†¡ ì¤‘...")

        # 4. PC1ìœ¼ë¡œ ì´ë¯¸ì§€ HTTP ì „ì†¡
        file_size = len(image_bytes)
        upload_url = f"{MASTER_API_URL}/image/internal/upload"
        upload_response = http_requests.post(
            upload_url,
            files={"file": (file_name, image_bytes, "image/png")},
            data={"image_id": image_id},
            timeout=30
        )
        if upload_response.status_code != 200:
            raise RuntimeError(f"PC1 ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨: {upload_response.status_code} - {upload_response.text}")

        upload_result = upload_response.json()
        file_path = upload_result.get("file_path", f"/app/uploads/images/{file_name}")
        print(f"âœ… [Worker] PC1 ì´ë¯¸ì§€ ì „ì†¡ ì™„ë£Œ: {file_name} ({file_size} bytes)")

        _update_task_progress("image", task_id, 90, "PC1 ì €ì¥ ì™„ë£Œ")

        # 5. DB ì—…ë°ì´íŠ¸
        _update_task_progress("image", task_id, 95, "ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘...")
        if user_id:
            db = SessionLocal()
            try:
                image_record = db.query(models.GeneratedImage).filter(
                    models.GeneratedImage.img_file == file_name
                ).first()
                if image_record:
                    image_record.img_size = file_size
                    db.commit()
            except Exception as db_err:
                print(f"âš ï¸ [Worker] DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {db_err}")
                db.rollback()
            finally:
                db.close()

        _update_task_progress("image", task_id, 100, "ì´ë¯¸ì§€ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!", "completed")
        return {"status": "completed", "file_path": file_path, "file_name": file_name, "file_size": file_size}

    except Exception as e:
        error_str = str(e)

        # ComfyUI í¬ë˜ì‹œ â†’ ìë™ ì¬ì‹œë„
        comfyui_crash_keywords = ['resolve', 'Connection', 'refused', 'lost', 'RemoteDisconnected']
        is_comfyui_crash = any(kw.lower() in error_str.lower() for kw in comfyui_crash_keywords)

        if is_comfyui_crash:
            print(f"âš ï¸ [Worker] ComfyUI ì—°ê²° ì‹¤íŒ¨ - 30ì´ˆ í›„ ì¬ì‹œë„")
            _update_task_progress("image", task_id, 5, "ComfyUI ì¬ì—°ê²° ëŒ€ê¸° ì¤‘... (ìë™ ì¬ì‹œë„)")
            try:
                raise self.retry(countdown=30)
            except self.MaxRetriesExceededError:
                pass

        error_msg = f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {error_str}"
        print(f"ğŸ”¥ [Worker] {error_msg}")
        _update_task_progress("image", task_id, 0, error_msg, "failed")
        return {"status": "failed", "error": error_msg}

    finally:
        after_task("image")


# =====================================================================
# GPU ìœ íœ´ ìì› í•´ì œ Task
# =====================================================================

@celery_app.task(name="release_gpu_if_idle_task")
def release_gpu_if_idle_task():
    """ìœ íœ´ GPU ìì› ìë™ í•´ì œ (Celery Beat ì£¼ê¸°ì  í˜¸ì¶œ)"""
    return release_if_idle()


# =====================================================================
# STT (Speech-to-Text) Task - Faster Whisper
# =====================================================================

@celery_app.task(name="transcribe_audio_task", bind=True, max_retries=20)
def transcribe_audio_task(self, meeting_id: int, audio_filename: str, language: str = "ko"):
    """íšŒì˜ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Faster Whisper + LLM ìš”ì•½)"""
    from worker.gpu_manager import get_stt_model

    task_id = self.request.id
    print(f"ğŸ¤ [Worker] STT ì‘ì—… ì‹œì‘ (Task ID: {task_id}, Meeting: {meeting_id})")

    # GPU ìì› íšë“
    if not try_acquire("stt"):
        print(f"â³ [Worker] GPU ì‚¬ìš© ì¤‘ - {GPU_RETRY_COUNTDOWN}ì´ˆ í›„ ì¬ì‹œë„")
        raise self.retry(countdown=GPU_RETRY_COUNTDOWN)

    _update_task_progress("stt", task_id, 5, "ìŒì„± ë³€í™˜ ì¤€ë¹„ ì¤‘...")

    db = SessionLocal()
    tmp_path = None
    try:
        # DB ìƒíƒœë¥¼ PROCESSINGìœ¼ë¡œ ì—…ë°ì´íŠ¸
        meeting = db.query(models.MeetingNote).filter(models.MeetingNote.id == meeting_id).first()
        if meeting:
            meeting.status = "PROCESSING"
            db.commit()

        # 1. PC1ì—ì„œ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
        _update_task_progress("stt", task_id, 10, "ì˜¤ë””ì˜¤ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        download_url = f"{MASTER_API_URL}/meeting/internal/file/{audio_filename}"
        resp = http_requests.get(download_url, timeout=120)
        if resp.status_code != 200:
            raise RuntimeError(f"ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {resp.status_code} - {resp.text}")

        ext = audio_filename.rsplit('.', 1)[-1] if '.' in audio_filename else 'wav'
        with tempfile.NamedTemporaryFile(suffix=f".{ext}", delete=False) as tmp:
            tmp.write(resp.content)
            tmp_path = tmp.name

        _update_task_progress("stt", task_id, 20, "ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

        # 2. STT ëª¨ë¸ ë¡œë“œ
        _update_task_progress("stt", task_id, 25, "STT ëª¨ë¸ ë¡œë”© ì¤‘...")
        model = get_stt_model()
        if model is None:
            raise RuntimeError("STT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")

        # 3. ìŒì„± ì¸ì‹
        _update_task_progress("stt", task_id, 35, "ìŒì„± ì¸ì‹ ì‹œì‘...")
        segments, info = model.transcribe(tmp_path, language=language, beam_size=5, vad_filter=True)

        # 4. íƒ€ì„ìŠ¤íƒ¬í”„ í¬ë§· ë³€í™˜
        _update_task_progress("stt", task_id, 50, "ìŒì„± ì¸ì‹ ì²˜ë¦¬ ì¤‘...")
        transcript_lines = []
        total_duration = 0
        segment_count = 0

        for segment in segments:
            start_mm, start_ss = int(segment.start) // 60, int(segment.start) % 60
            end_mm, end_ss = int(segment.end) // 60, int(segment.end) % 60
            text = segment.text.strip()

            if text:
                transcript_lines.append(f"[{start_mm:02d}:{start_ss:02d} ~ {end_mm:02d}:{end_ss:02d}] {text}")

            total_duration = max(total_duration, int(segment.end))
            segment_count += 1

            if segment_count % 10 == 0:
                progress = min(50 + segment_count // 2, 75)
                _update_task_progress("stt", task_id, progress, f"ìŒì„± ì¸ì‹ ì¤‘... ({segment_count}ê°œ ì„¸ê·¸ë¨¼íŠ¸)")

        transcript = "\n".join(transcript_lines)
        print(f"âœ… [Worker] STT ì™„ë£Œ: {segment_count}ê°œ ì„¸ê·¸ë¨¼íŠ¸, {total_duration}ì´ˆ")

        # 5. LLM ìš”ì•½ ìƒì„±
        _update_task_progress("stt", task_id, 80, "AIê°€ íšŒì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        meeting_summary = _generate_meeting_summary(transcript)

        # 6. DB ì—…ë°ì´íŠ¸
        _update_task_progress("stt", task_id, 90, "ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘...")
        meeting = db.query(models.MeetingNote).filter(models.MeetingNote.id == meeting_id).first()
        if meeting:
            meeting.transcript = transcript
            meeting.duration = total_duration
            meeting.status = "COMPLETED"
            if meeting_summary:
                meeting.summary = meeting_summary
            db.commit()

        _update_task_progress("stt", task_id, 100, "ìŒì„± ë³€í™˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!", "completed")
        return {"status": "completed", "meeting_id": meeting_id, "segments": segment_count, "duration": total_duration}

    except Exception as e:
        error_msg = f"STT ë³€í™˜ ì‹¤íŒ¨: {str(e)}"
        print(f"ğŸ”¥ [Worker] {error_msg}")
        _update_task_progress("stt", task_id, 0, f"ìŒì„± ë³€í™˜ ì‹¤íŒ¨: {str(e)}", "failed")

        try:
            meeting = db.query(models.MeetingNote).filter(models.MeetingNote.id == meeting_id).first()
            if meeting:
                meeting.status = "ERROR"
                db.commit()
        except Exception as db_err:
            print(f"âš ï¸ [Worker] DB ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {db_err}")

        return {"status": "failed", "error": error_msg}

    finally:
        if tmp_path and os.path.exists(tmp_path):
            os.unlink(tmp_path)
        db.close()
        after_task("stt")
