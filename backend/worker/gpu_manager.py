"""
GPU VRAM ë™ì  ìì› ê´€ë¦¬ì (ë°°ì¹˜ ì¸ì‹ ìŠ¤ì¼€ì¤„ë§)

PC2 Workerì—ì„œ GPU VRAMì„ ê³µìœ í•˜ëŠ” ëª¨ë¸ë“¤ì˜ ë¡œë“œ/ì–¸ë¡œë“œë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
ë¶ˆí•„ìš”í•œ ëª¨ë¸ ì „í™˜ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ëª¨ë¸ ë¶„ë¥˜:
    - ìƒì‹œ ë¡œë“œ: ì„ë² ë”© ëª¨ë¸ (~0.2~1.2GB) - ì´ ê´€ë¦¬ì ëŒ€ìƒ ì•„ë‹˜
    - ë™ì  ë¡œë“œ: ComfyUI ì´ë¯¸ì§€ (SD 3.5) ~4.5GB
    - ë™ì  ë¡œë“œ: Faster Whisper STT ~3.5GB

ìŠ¤ì¼€ì¤„ë§ ì •ì±…:
    - ê°™ì€ íƒ€ì… ì‘ì—…: ëª¨ë¸ ì „í™˜ ì—†ì´ ì¦‰ì‹œ ì‹¤í–‰
    - ë‹¤ë¥¸ íƒ€ì… ì‘ì—…: í˜„ì¬ ë°°ì¹˜(ìµœëŒ€ 5ê°œ)ê°€ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
    - ë°°ì¹˜ í•œë„ ë„ë‹¬ OR í˜„ì¬ íƒ€ì… ëŒ€ê¸° ì—†ìŒ: ëª¨ë¸ ì „í™˜ ì‹¤í–‰
    - ëŒ€ê¸° ì‘ì—… ì—†ìœ¼ë©´ ëª¨ë¸ ì–¸ë¡œë“œí•˜ì§€ ì•ŠìŒ

Celery í êµ¬ì¡°:
    - celery (ê¸°ë³¸): ì¼ë°˜ ì‘ì—… (ì±„íŒ… ì €ì¥, RAG ë“±)
    - gpu_image: ì´ë¯¸ì§€ ìƒì„± ì‘ì—…
    - gpu_stt: STT ìŒì„± ì¸ì‹ ì‘ì—…

ì‘ì„±ì¼: 2025
ì‘ì„±ì: DOT-Project Team
"""

import os
import gc
import time
import redis
import requests as http_requests

# Redis ì„¤ì •
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
redis_client = redis.from_url(REDIS_URL, decode_responses=True)

# ComfyUI ì„¤ì •
COMFYUI_HOST = os.environ.get("COMFYUI_HOST", "comfyui")
COMFYUI_PORT = os.environ.get("COMFYUI_PORT", "8188")
COMFYUI_BASE_URL = f"http://{COMFYUI_HOST}:{COMFYUI_PORT}"

# =====================================================================
# ì„¤ì •ê°’
# =====================================================================
GPU_MAX_BATCH = 5                           # ëª¨ë¸ ì „í™˜ ì „ ìµœëŒ€ ì—°ì† ì²˜ë¦¬ ìˆ˜
GPU_RETRY_COUNTDOWN = 5                     # ëŒ€ê¸° ì‹œ ì¬ì‹œë„ ê°„ê²© (ì´ˆ)

# Redis í‚¤
_KEY_ACTIVE_MODEL = "gpu:active_model"      # í˜„ì¬ í™œì„± ëª¨ë¸: "image" | "stt" | "none"
_KEY_BATCH_COUNT = "gpu:batch_count"        # í˜„ì¬ ëª¨ë¸ì˜ ì—°ì† ì²˜ë¦¬ ìˆ˜
_KEY_LAST_ACTIVITY = "gpu:last_activity"    # ë§ˆì§€ë§‰ GPU ì‚¬ìš© íƒ€ì„ìŠ¤íƒ¬í”„

# Celery í ì´ë¦„ (Redis í‚¤ì™€ ë™ì¼)
QUEUE_IMAGE = "gpu_image"
QUEUE_STT = "gpu_stt"

# ëª¨ë¸ íƒ€ì… â†’ í ì´ë¦„ ë§¤í•‘
_QUEUE_MAP = {
    "image": QUEUE_IMAGE,
    "stt": QUEUE_STT,
}

# STT ëª¨ë¸ ì‹±ê¸€í†¤ (Worker í”„ë¡œì„¸ìŠ¤ ë‚´)
_stt_model = None


# =====================================================================
# ë‚´ë¶€ ìƒíƒœ ê´€ë¦¬
# =====================================================================

def _get_active_model() -> str:
    try:
        return redis_client.get(_KEY_ACTIVE_MODEL) or "none"
    except Exception:
        return "none"


def _set_active_model(model_type: str):
    try:
        redis_client.set(_KEY_ACTIVE_MODEL, model_type)
    except Exception as e:
        print(f"âš ï¸ [GPU] Redis ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")


def _get_batch_count() -> int:
    try:
        val = redis_client.get(_KEY_BATCH_COUNT)
        return int(val) if val else 0
    except Exception:
        return 0


def _increment_batch():
    try:
        return redis_client.incr(_KEY_BATCH_COUNT)
    except Exception:
        return 0


def _reset_batch():
    try:
        redis_client.set(_KEY_BATCH_COUNT, 0)
    except Exception:
        pass


def _update_activity():
    try:
        redis_client.setex(_KEY_LAST_ACTIVITY, 120, str(time.time()))
    except Exception:
        pass


def _get_queue_length(queue_name: str) -> int:
    """Celery Redis íì˜ ëŒ€ê¸° ì‘ì—… ìˆ˜ ì¡°íšŒ"""
    try:
        return redis_client.llen(queue_name)
    except Exception:
        return 0


# =====================================================================
# ëª¨ë¸ ë¡œë“œ/ì–¸ë¡œë“œ (ComfyUI + STT)
# =====================================================================

def _free_comfyui_vram():
    """ComfyUI ì»¨í…Œì´ë„ˆì˜ VRAM í•´ì œ ìš”ì²­ (í•´ì œ ì™„ë£Œ í™•ì¸)"""
    try:
        print("ğŸ”„ [GPU] ComfyUI VRAM í•´ì œ ìš”ì²­ ì¤‘...")
        resp = http_requests.post(
            f"{COMFYUI_BASE_URL}/free",
            json={"free_memory": True},
            timeout=10
        )
        if resp.status_code == 200:
            print("âœ… [GPU] ComfyUI VRAM í•´ì œ ìš”ì²­ ì „ì†¡ ì™„ë£Œ")
        else:
            print(f"âš ï¸ [GPU] ComfyUI /free ì‘ë‹µ: {resp.status_code}")
            time.sleep(2)
            return

        # VRAM í•´ì œ ì™„ë£Œ í™•ì¸ (ìµœëŒ€ 30ì´ˆ, 2ì´ˆ ê°„ê²© í´ë§)
        for attempt in range(15):
            time.sleep(2)
            try:
                stats_resp = http_requests.get(
                    f"{COMFYUI_BASE_URL}/system_stats",
                    timeout=5
                )
                if stats_resp.status_code != 200:
                    continue

                devices = stats_resp.json().get("system", {}).get("devices", [])
                if not devices:
                    continue

                gpu = devices[0]
                vram_total = gpu.get("vram_total", 0)
                vram_free = gpu.get("vram_free", 0)

                if vram_total > 0:
                    vram_used_mb = (vram_total - vram_free) / (1024 * 1024)
                    free_pct = (vram_free / vram_total) * 100
                    print(f"ğŸ“Š [GPU] ComfyUI VRAM: {vram_used_mb:.0f}MB ì‚¬ìš© ì¤‘ "
                          f"({free_pct:.0f}% ì—¬ìœ , ì‹œë„ {attempt + 1}/15)")

                    # VRAM ì‚¬ìš©ëŸ‰ì´ 1GB ë¯¸ë§Œì´ë©´ í•´ì œ ì™„ë£Œë¡œ íŒë‹¨
                    if vram_used_mb < 1024:
                        print("âœ… [GPU] ComfyUI VRAM í•´ì œ í™•ì¸ ì™„ë£Œ")
                        return
            except Exception:
                continue

        print("âš ï¸ [GPU] ComfyUI VRAM í•´ì œ í™•ì¸ íƒ€ì„ì•„ì›ƒ (30ì´ˆ) - ê³„ì† ì§„í–‰")

    except http_requests.exceptions.ConnectionError:
        print("âš ï¸ [GPU] ComfyUI ì—°ê²° ë¶ˆê°€ (ì»¨í…Œì´ë„ˆ ë¯¸ì‹¤í–‰?)")
    except Exception as e:
        print(f"âš ï¸ [GPU] ComfyUI VRAM í•´ì œ ì‹¤íŒ¨: {e}")


def _load_stt_model():
    """Faster Whisper STT ëª¨ë¸ì„ GPUì— ë¡œë“œ"""
    global _stt_model
    if _stt_model is not None:
        return _stt_model

    try:
        from faster_whisper import WhisperModel
        print("ğŸ“¥ [GPU] Faster Whisper ëª¨ë¸ ë¡œë”© ì¤‘... (GPU)")
        _clear_cuda_cache()

        # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ (íì‡„ë§ - ì™¸ë¶€ ë‹¤ìš´ë¡œë“œ ë¶ˆê°€)
        model_path = os.getenv("STT_MODEL_PATH", "/models/faster-whisper-large-v3")
        _stt_model = WhisperModel(
            model_path,
            device="cuda",
            compute_type="int8"
        )
        print("âœ… [GPU] Faster Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        return _stt_model

    except ImportError:
        print("âš ï¸ [GPU] faster-whisper íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜")
        return None
    except Exception as e:
        print(f"ğŸ”¥ [GPU] STT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None


def _unload_stt_model():
    """Faster Whisper STT ëª¨ë¸ì„ VRAMì—ì„œ ì–¸ë¡œë“œ"""
    global _stt_model
    if _stt_model is None:
        return
    try:
        print("ğŸ”„ [GPU] Faster Whisper ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
        del _stt_model
        _stt_model = None
        _clear_cuda_cache()
        print("âœ… [GPU] Faster Whisper ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        _stt_model = None
        print(f"âš ï¸ [GPU] STT ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")


def _clear_cuda_cache():
    """PyTorch CUDA ìºì‹œ ì •ë¦¬"""
    try:
        gc.collect()
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
    except Exception:
        pass


def _switch_to(model_type: str):
    """
    GPU ëª¨ë¸ ì „í™˜

    í˜„ì¬ ëª¨ë¸ì„ ì–¸ë¡œë“œí•˜ê³  ìƒˆ ëª¨ë¸ íƒ€ì…ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.
    ComfyUIëŠ” /prompt ìš”ì²­ ì‹œ ìë™ ë¡œë“œë˜ë¯€ë¡œ ì—¬ê¸°ì„œ ì§ì ‘ ë¡œë“œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    STTëŠ” ëª…ì‹œì ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    current = _get_active_model()
    print(f"ğŸ”„ [GPU] ëª¨ë¸ ì „í™˜: {current} â†’ {model_type}")

    # í˜„ì¬ ëª¨ë¸ ì–¸ë¡œë“œ
    if current == "image":
        _free_comfyui_vram()
    elif current == "stt":
        _unload_stt_model()

    # ìƒˆ ëª¨ë¸ ì„¤ì •
    _set_active_model(model_type)
    _reset_batch()
    _update_activity()

    # STTëŠ” ëª…ì‹œì  ë¡œë“œ í•„ìš” (ComfyUIëŠ” ìš”ì²­ ì‹œ ìë™ ë¡œë“œ)
    if model_type == "stt":
        _load_stt_model()


# =====================================================================
# ê³µê°œ API: ë°°ì¹˜ ì¸ì‹ GPU íšë“
# =====================================================================

def try_acquire(task_type: str) -> bool:
    """
    GPU ìì› íšë“ ì‹œë„ (ë°°ì¹˜ ì¸ì‹ ìŠ¤ì¼€ì¤„ë§)

    Args:
        task_type: "image" ë˜ëŠ” "stt"

    Returns:
        True: íšë“ ì„±ê³µ, ì‘ì—… ì§„í–‰ ê°€ëŠ¥
        False: íšë“ ì‹¤íŒ¨, ë‹¤ë¥¸ ëª¨ë¸ì˜ ë°°ì¹˜ê°€ ì§„í–‰ ì¤‘ì´ë¯€ë¡œ ë‚˜ì¤‘ì— ì¬ì‹œë„

    ìŠ¤ì¼€ì¤„ë§ ì •ì±…:
        1. ê°™ì€ ëª¨ë¸ â†’ ì¦‰ì‹œ ì§„í–‰ (ë°°ì¹˜ ì¹´ìš´í„° ì¦ê°€)
        2. ëª¨ë¸ ì—†ìŒ â†’ ë¡œë“œ í›„ ì§„í–‰
        3. ë‹¤ë¥¸ ëª¨ë¸ + ë°°ì¹˜ í•œë„ ë¯¸ë‹¬ + ëŒ€ê¸° ì‘ì—… ìˆìŒ â†’ ê±°ë¶€ (ì¬ì‹œë„)
        4. ë‹¤ë¥¸ ëª¨ë¸ + ë°°ì¹˜ í•œë„ ë„ë‹¬ OR ëŒ€ê¸° ì—†ìŒ â†’ ì „í™˜ í›„ ì§„í–‰
    """
    current = _get_active_model()

    # Case 1: ê°™ì€ ëª¨ë¸ì´ ì´ë¯¸ í™œì„± â†’ ì¦‰ì‹œ ì§„í–‰
    if current == task_type:
        count = _increment_batch()
        _update_activity()
        print(f"âœ… [GPU] {task_type} ì‘ì—… ì§„í–‰ (ë°°ì¹˜ {count}/{GPU_MAX_BATCH})")
        return True

    # Case 2: GPU ë¹„ì–´ìˆìŒ â†’ ìƒˆ ëª¨ë¸ ë¡œë“œ
    if current == "none":
        _switch_to(task_type)
        _increment_batch()
        print(f"âœ… [GPU] {task_type} ëª¨ë¸ ìƒˆë¡œ ë¡œë“œ (ë°°ì¹˜ 1/{GPU_MAX_BATCH})")
        return True

    # Case 3 & 4: ë‹¤ë¥¸ ëª¨ë¸ì´ í™œì„± ì¤‘
    current_queue = _QUEUE_MAP.get(current, "")
    current_pending = _get_queue_length(current_queue)
    current_batch = _get_batch_count()

    if current_batch < GPU_MAX_BATCH and current_pending > 0:
        # Case 3: í˜„ì¬ ëª¨ë¸ ë°°ì¹˜ í•œë„ ë¯¸ë‹¬ + ëŒ€ê¸° ì‘ì—… ìˆìŒ â†’ ëŒ€ê¸°
        print(f"â³ [GPU] {task_type} ëŒ€ê¸° - {current} ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ "
              f"({current_batch}/{GPU_MAX_BATCH}, ëŒ€ê¸° {current_pending}ê°œ)")
        return False

    # Case 4: ë°°ì¹˜ í•œë„ ë„ë‹¬ OR ëŒ€ê¸° ì‘ì—… ì—†ìŒ â†’ ì „í™˜
    reason = "ë°°ì¹˜ í•œë„ ë„ë‹¬" if current_batch >= GPU_MAX_BATCH else "ëŒ€ê¸° ì‘ì—… ì—†ìŒ"
    print(f"ğŸ”„ [GPU] ëª¨ë¸ ì „í™˜ ê²°ì • ({reason}): {current} â†’ {task_type}")
    _switch_to(task_type)
    _increment_batch()
    return True


def _cleanup_comfyui_cache():
    """ComfyUI ë‚´ë¶€ ìºì‹œ ê²½ëŸ‰ ì •ë¦¬ (ëª¨ë¸ì€ ìœ ì§€, ì¤‘ê°„ í…ì„œë§Œ í•´ì œ)"""
    try:
        resp = http_requests.post(
            f"{COMFYUI_BASE_URL}/free",
            json={"free_memory": True},
            timeout=10
        )
        if resp.status_code == 200:
            print("ğŸ§¹ [GPU] ComfyUI ë‚´ë¶€ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
    except Exception:
        pass


def after_task(task_type: str):
    """
    GPU ì‘ì—… ì™„ë£Œ í›„ í˜¸ì¶œ

    ë°°ì¹˜ í•œë„ì— ë„ë‹¬í–ˆê³  ë‹¤ë¥¸ íƒ€ì…ì˜ ëŒ€ê¸° ì‘ì—…ì´ ìˆìœ¼ë©´
    í˜„ì¬ ëª¨ë¸ì„ ì–¸ë¡œë“œí•˜ì—¬ ë‹¤ìŒ ì‘ì—…ì´ ë¹ ë¥´ê²Œ ì „í™˜í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
    ëŒ€ê¸° ì‘ì—…ì´ ì—†ìœ¼ë©´ í˜„ì¬ ëª¨ë¸ì„ ìœ ì§€í•©ë‹ˆë‹¤.

    Args:
        task_type: ì™„ë£Œëœ ì‘ì—… íƒ€ì… ("image" ë˜ëŠ” "stt")
    """
    _update_activity()
    current_batch = _get_batch_count()

    # ì´ë¯¸ì§€ ì‘ì—… í›„ ComfyUI ë‚´ë¶€ ìºì‹œ ì •ë¦¬ (ì—°ì† ì‹¤í–‰ ì‹œ ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€)
    if task_type == "image":
        _cleanup_comfyui_cache()

    if current_batch >= GPU_MAX_BATCH:
        # ë°°ì¹˜ í•œë„ ë„ë‹¬ â†’ ë‹¤ë¥¸ íƒ€ì… ëŒ€ê¸° ì‘ì—… í™•ì¸
        other_type = "stt" if task_type == "image" else "image"
        other_queue = _QUEUE_MAP.get(other_type, "")
        other_pending = _get_queue_length(other_queue)

        if other_pending > 0:
            # ë‹¤ë¥¸ íƒ€ì… ëŒ€ê¸° ì¤‘ â†’ ë¯¸ë¦¬ ì–¸ë¡œë“œí•˜ì—¬ ì „í™˜ ì¤€ë¹„
            print(f"ğŸ“‹ [GPU] ë°°ì¹˜ {current_batch}ê°œ ì™„ë£Œ, "
                  f"{other_type} ëŒ€ê¸° {other_pending}ê°œ â†’ ì‚¬ì „ ì–¸ë¡œë“œ")
            if task_type == "image":
                _free_comfyui_vram()
            elif task_type == "stt":
                _unload_stt_model()
            _set_active_model("none")
            _reset_batch()
        else:
            # ëŒ€ê¸° ì‘ì—… ì—†ìŒ â†’ í˜„ì¬ ëª¨ë¸ ìœ ì§€ (ë¶ˆí•„ìš”í•œ ì „í™˜ ë°©ì§€)
            print(f"ğŸ“‹ [GPU] ë°°ì¹˜ {current_batch}ê°œ ì™„ë£Œ, "
                  f"ëŒ€ê¸° ì‘ì—… ì—†ìŒ â†’ {task_type} ëª¨ë¸ ìœ ì§€")
            _reset_batch()  # ì¹´ìš´í„°ë§Œ ë¦¬ì…‹


def release_if_idle():
    """
    ìœ íœ´ GPU ìì› ìë™ í•´ì œ (Celery Beatì—ì„œ ì£¼ê¸°ì  í˜¸ì¶œ)

    ì–‘ìª½ í ëª¨ë‘ ëŒ€ê¸° ì‘ì—…ì´ ì—†ê³  íƒ€ì„ì•„ì›ƒì´ ê²½ê³¼í•˜ë©´ VRAMì„ í•´ì œí•©ë‹ˆë‹¤.
    ëŒ€ê¸° ì‘ì—…ì´ ìˆìœ¼ë©´ í•´ì œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

    Returns:
        dict: í•´ì œ ê²°ê³¼
    """
    current = _get_active_model()
    if current == "none":
        return {"status": "idle"}

    # ì–´ëŠ ìª½ì´ë“  ëŒ€ê¸° ì‘ì—…ì´ ìˆìœ¼ë©´ í•´ì œí•˜ì§€ ì•ŠìŒ
    image_pending = _get_queue_length(QUEUE_IMAGE)
    stt_pending = _get_queue_length(QUEUE_STT)

    if current == "image" and image_pending > 0:
        return {"status": "active", "model": "image", "pending": image_pending}
    if current == "stt" and stt_pending > 0:
        return {"status": "active", "model": "stt", "pending": stt_pending}

    # ëŒ€ê¸° ì‘ì—… ì—†ìŒ â†’ íƒ€ì„ì•„ì›ƒ í™•ì¸
    try:
        last_activity = redis_client.get(_KEY_LAST_ACTIVITY)
        if last_activity:
            elapsed = time.time() - float(last_activity)
            if elapsed < 30:
                return {"status": "waiting", "model": current, "idle": round(elapsed)}
    except Exception:
        pass

    # íƒ€ì„ì•„ì›ƒ ê²½ê³¼ + ëŒ€ê¸° ì—†ìŒ â†’ í•´ì œ
    print(f"â° [GPU] ìœ íœ´ íƒ€ì„ì•„ì›ƒ â†’ {current} ëª¨ë¸ í•´ì œ")
    if current == "image":
        _free_comfyui_vram()
    elif current == "stt":
        _unload_stt_model()
    _set_active_model("none")
    _reset_batch()
    return {"status": "released", "model": current}


def get_stt_model():
    """í˜„ì¬ ë¡œë“œëœ STT ëª¨ë¸ ë°˜í™˜ (ë¯¸ë¡œë“œ ì‹œ ë¡œë“œ ì‹œë„)"""
    if _stt_model is None:
        return _load_stt_model()
    return _stt_model


def get_status() -> dict:
    """GPU ê´€ë¦¬ì í˜„ì¬ ìƒíƒœ ì¡°íšŒ (ë””ë²„ê¹…ìš©)"""
    return {
        "active_model": _get_active_model(),
        "batch_count": _get_batch_count(),
        "max_batch": GPU_MAX_BATCH,
        "queue_image_pending": _get_queue_length(QUEUE_IMAGE),
        "queue_stt_pending": _get_queue_length(QUEUE_STT),
    }
