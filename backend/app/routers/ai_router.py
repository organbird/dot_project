"""
AI Router - AI ì±„íŒ…, ìŠ¤íŠ¸ë¦¬ë°, PDF ì—…ë¡œë“œ ì²˜ë¦¬
"""

from fastapi import APIRouter, UploadFile, File, HTTPException, Depends
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from sqlalchemy.orm import Session
from app import models
from app.database import get_db
from app.config import redis_client
import shutil
import json
import os
import threading
import time
import uuid

from ai_core.llm_engine import LLMEngine, llm_lock
from ai_core.rag_engine import RAGEngine
from worker.tasks import ingest_pdf_task, save_chat_task, update_summary_task

router = APIRouter(prefix="/ai", tags=["AI Core"])

llm = LLMEngine()
rag = RAGEngine()


# Pydantic ìš”ì²­ ëª¨ë¸
class ChatRequest(BaseModel):
    message: str

class ChatStreamRequest(BaseModel):
    session_id: int
    message: str
    history: list = []

class SummaryUpdateRequest(BaseModel):
    oldest_message_ids: list[int]

class ChatStopRequest(BaseModel):
    session_id: int


def load_ai_models():
    """ì„œë²„ ì‹œì‘ ì‹œ LLM ëª¨ë¸ ë¡œë”©"""
    print("ğŸš€ [AI Router] LLM ëª¨ë¸ ë¡œë”© ì‹œì‘...")
    try:
        llm.load_model()
        print("âœ… [AI Router] ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    except Exception as e:
        print(f"ğŸ”¥ [AI Router] ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")


@router.post("/chat")
async def chat_endpoint(req: ChatRequest):
    """RAG ê¸°ë°˜ ì¼ë°˜ ì±„íŒ… (ë¹„ìŠ¤íŠ¸ë¦¬ë°, ì™„ì„±ëœ ì‘ë‹µ í•œ ë²ˆì— ë°˜í™˜)"""
    user_msg = req.message
    print(f"ğŸ“© [User] {user_msg}")

    search_results = rag.search(user_msg, k=3)

    if search_results:
        print(f"ğŸ” [RAG] ê´€ë ¨ ë¬¸ì„œ {len(search_results)}ê°œ ë°œê²¬")
        context_text = "\n".join([res['content'] for res in search_results])
        final_prompt = f"""
        [ì§€ì‹œì‚¬í•­]
        ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ë˜, ì•„ë˜ [ì°¸ê³  ìë£Œ]ë¥¼ í™œìš©í•˜ì„¸ìš”.

        â˜…ì¤‘ìš”â˜…: ë§Œì•½ [ì°¸ê³  ìë£Œ]ê°€ ì§ˆë¬¸ê³¼ ì „í˜€ ê´€ë ¨ì´ ì—†ë‹¤ë©´, ìë£Œë¥¼ ë¬´ì‹œí•˜ê³  ë‹¹ì‹ ì˜ ë°°ê²½ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
        ìë£Œë¥¼ ì–µì§€ë¡œ ì—°ê²°ì§“ì§€ ë§ˆì„¸ìš”. ë°˜ëŒ€ë¡œ [ì°¸ê³  ìë£Œ]ê°€ ê´€ë ¨ì„±ì´ ìˆë‹¤ë©´ ì´ ìë£Œ ë‚´ì—ì„œ ë‹µë³€í•˜ê³ , ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.

        [ì°¸ê³  ìë£Œ]
        {context_text}

        [ì§ˆë¬¸]
        {user_msg}
        """
    else:
        print("ğŸ¤·â€â™‚ï¸ [RAG] ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ")
        final_prompt = f"""
        [ì§€ì‹œì‚¬í•­]
        ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.

        [ì§ˆë¬¸]
        {user_msg}
        """

    llm.ensure_loaded()
    response = llm.chat(final_prompt)
    return {"reply": response, "context_used": search_results}


@router.get("/chat/sessions/{session_id}/messages")
def get_chat_history(session_id: int, db: Session = Depends(get_db)):
    """ì„¸ì…˜ ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¡°íšŒ (Redis ìºì‹œ â†’ MySQL í´ë°±)"""
    redis_key = f"session:{session_id}:context"

    cached_context = redis_client.get(redis_key)
    if cached_context:
        print(f"âš¡ [Cache Hit] ì„¸ì…˜ {session_id} - Redisì—ì„œ ë¡œë“œ")
        return json.loads(cached_context)

    print(f"ğŸ¢ [Cache Miss] ì„¸ì…˜ {session_id} - DBì—ì„œ ì¡°íšŒ")

    session = db.query(models.ChatSession)\
        .filter(models.ChatSession.id == session_id)\
        .first()

    if not session:
        return {"summary": None, "messages": []}

    db_messages = db.query(models.ChatMessage)\
        .filter(models.ChatMessage.session_id == session_id)\
        .order_by(models.ChatMessage.created_at.desc())\
        .limit(10)\
        .all()

    db_messages = list(reversed(db_messages))

    messages_list = [
        {"sender": "user" if msg.sender == "user" else "assistant", "content": msg.content}
        for msg in db_messages
    ]

    result = {"summary": session.current_summary, "messages": messages_list}

    redis_client.setex(redis_key, 3600, json.dumps(result, ensure_ascii=False))
    print(f"âœ… [Cache Refill] ì„¸ì…˜ {session_id} - ìš”ì•½ + ìµœê·¼ {len(messages_list)}ê°œ ë©”ì‹œì§€ ì €ì¥")

    return result


def background_producer(session_id: int, user_msg: str, final_input: str, history: list, search_results: list):
    """ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ LLM ì‘ë‹µ ìƒì„± â†’ Redis í í‘¸ì‹œ (Producer)"""
    stream_key = f"session:{session_id}:stream_queue"
    stop_key = f"session:{session_id}:stop"

    redis_client.delete(stream_key)
    full_ai_response = ""
    is_stopped = False

    print(f"ğŸ‘» [Thread] ì„¸ì…˜ {session_id} ìƒì„± ì‹œì‘")

    try:
        if search_results:
            docs_json = json.dumps(search_results, ensure_ascii=False)
            redis_client.rpush(stream_key, f"DOCS:{docs_json}")

        llm.ensure_loaded()

        for token in llm.chat_stream(final_input, history):
            if redis_client.exists(stop_key):
                print(f"ğŸ›‘ [Thread] ì¤‘ë‹¨ ì‹ í˜¸ ê°ì§€!")
                is_stopped = True
                break
            full_ai_response += token
            redis_client.rpush(stream_key, f"TEXT:{token}")

    except Exception as e:
        print(f"ğŸ”¥ [Thread] ìƒì„± ì¤‘ ì—ëŸ¬: {e}")
        redis_client.rpush(stream_key, f"ERROR:{str(e)}")
        return

    if is_stopped:
        redis_client.rpush(stream_key, "STOPPED")
        redis_client.delete(stop_key)
        print("ğŸ—‘ï¸ [Thread] ì‘ì—… íê¸° (DB ì €ì¥ ì•ˆí•¨)")
        return

    redis_client.rpush(stream_key, "DONE")

    print(f"ğŸ’¾ [Thread] ìƒì„± ì™„ë£Œ. Celeryì—ê²Œ ì €ì¥ ìš”ì²­ (ê¸¸ì´: {len(full_ai_response)})")
    ref_json = json.dumps(search_results, ensure_ascii=False) if search_results else None
    save_chat_task.delay(
        session_id=session_id,
        user_msg=user_msg,
        ai_msg=full_ai_response,
        ref_docs_json=ref_json
    )

    redis_client.expire(stream_key, 60)


@router.post("/chat/stream")
async def chat_stream_endpoint(req: ChatStreamRequest):
    """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… (SSE, Producer-Consumer íŒ¨í„´)"""
    session_id = req.session_id
    user_msg = req.message

    search_results = rag.search(user_msg, k=3)

    if search_results:
        context_text = "\n".join([res['content'] for res in search_results])
        final_input = f"""[ì°¸ê³  ìë£Œ]\n{context_text}\n\n[ì§ˆë¬¸]\n{user_msg}\n\nìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."""
    else:
        final_input = user_msg

    t = threading.Thread(
        target=background_producer,
        args=(session_id, user_msg, final_input, req.history, search_results),
        daemon=True
    )
    t.start()

    def event_consumer():
        """Redis íì—ì„œ í† í°ì„ ì½ì–´ SSEë¡œ ìŠ¤íŠ¸ë¦¬ë°"""
        stream_key = f"session:{session_id}:stream_queue"
        last_activity = time.time()

        while True:
            if time.time() - last_activity > 30:
                print("â±ï¸ [Consumer] íƒ€ì„ì•„ì›ƒ")
                break

            item = redis_client.blpop(stream_key, timeout=1)

            if item:
                last_activity = time.time()
                _, value = item

                if value == "DONE":
                    break
                if value == "STOPPED":
                    yield f"STOPPED_DATA:\n\n"
                    break
                if value.startswith("DOCS:"):
                    yield f"DOCS_DATA:{value[5:]}\n\n"
                elif value.startswith("TEXT:"):
                    yield f"TEXT_DATA:{value[5:]}\n\n"
                elif value.startswith("ERROR:"):
                    yield f"ERROR_DATA:{value[6:]}\n\n"
                    break

    return StreamingResponse(event_consumer(), media_type="text/event-stream")


@router.post("/chat/stop")
async def stop_chat_generation(req: ChatStopRequest):
    """ì‹¤í–‰ ì¤‘ì¸ ì±„íŒ… ìƒì„± ì¤‘ë‹¨ (ì¤‘ë‹¨ í”Œë˜ê·¸ â†’ ìƒì‚°ì ìŠ¤ë ˆë“œ ì¢…ë£Œ)"""
    stop_key = f"session:{req.session_id}:stop"
    redis_client.set(stop_key, "1", ex=60)

    stream_key = f"session:{req.session_id}:stream_queue"
    redis_client.delete(stream_key)
    redis_client.rpush(stream_key, "STOPPED")

    print(f"ğŸ›‘ [Stop] ì„¸ì…˜ {req.session_id} ì¤‘ë‹¨ ìš”ì²­ ì ‘ìˆ˜")
    return {"status": "stopped"}


@router.post("/upload")
async def upload_pdf(file: UploadFile = File(...)):
    """PDF ì—…ë¡œë“œ í›„ Celery Workerì—ì„œ ë²¡í„°DB ì„ë² ë”© ì²˜ë¦¬"""
    save_dir = "/ai_models/uploads"
    os.makedirs(save_dir, exist_ok=True)

    file_path = os.path.join(save_dir, file.filename)
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    task = ingest_pdf_task.delay(file_path)
    print(f"ğŸ‘‹ [Backend] Workerì—ê²Œ ì‘ì—… ì „ë‹¬ ì™„ë£Œ (Task ID: {task.id})")

    return {
        "filename": file.filename,
        "status": "Processing started in background (Worker)",
        "task_id": task.id
    }


@router.post("/sessions/{session_id}/update-summary")
async def update_session_summary(session_id: int, req: SummaryUpdateRequest, db: Session = Depends(get_db)):
    """ì„¸ì…˜ ìš”ì•½ ì¬ìƒì„± (ê°€ì¥ ì˜¤ë˜ëœ 2ê°œ ë©”ì‹œì§€ + ê¸°ì¡´ ìš”ì•½ â†’ Celery Worker)"""
    session = db.query(models.ChatSession)\
        .filter(models.ChatSession.id == session_id)\
        .first()

    if not session:
        raise HTTPException(status_code=404, detail=f"Session {session_id} not found")

    oldest_messages = db.query(models.ChatMessage)\
        .filter(models.ChatMessage.id.in_(req.oldest_message_ids))\
        .order_by(models.ChatMessage.created_at.asc())\
        .all()

    if len(oldest_messages) < 2:
        raise HTTPException(status_code=400, detail=f"Need at least 2 messages (found {len(oldest_messages)})")

    messages_list = [{"sender": msg.sender, "content": msg.content} for msg in oldest_messages]

    task = update_summary_task.delay(
        session_id=session_id,
        current_summary=session.current_summary,
        oldest_messages=messages_list
    )

    print(f"ğŸ”„ [API] ì„¸ì…˜ {session_id} ìš”ì•½ ì—…ë°ì´íŠ¸ ìš”ì²­ (Task: {task.id})")
    return {"status": "processing", "task_id": task.id, "message": f"Summary update started for session {session_id}"}


@router.post("/chat/generate")
async def generate_chat_background(req: ChatRequest):
    """ë°±ê·¸ë¼ìš´ë“œ LLM ìƒì„± (Worker PCê°€ í˜¸ì¶œ, ê²°ê³¼ëŠ” Redisì— ì €ì¥)"""
    task_id = str(uuid.uuid4())

    def run_llm_background():
        try:
            print(f"ğŸš€ [Background] LLM ìƒì„± ì‹œì‘ (Task: {task_id})")
            result = llm.chat(req.message)
            redis_client.setex(
                f"llm_result:{task_id}", 300,
                json.dumps({"result": result, "status": "completed"}, ensure_ascii=False)
            )
            print(f"âœ… [Background] LLM ìƒì„± ì™„ë£Œ (Task: {task_id})")
        except Exception as e:
            error_msg = str(e)
            print(f"ğŸ”¥ [Background] LLM ìƒì„± ì‹¤íŒ¨ (Task: {task_id}): {error_msg}")
            redis_client.setex(
                f"llm_result:{task_id}", 300,
                json.dumps({"error": error_msg, "status": "failed"}, ensure_ascii=False)
            )

    thread = threading.Thread(target=run_llm_background, daemon=True)
    thread.start()

    print(f"ğŸ“¤ [API] LLM ì‘ì—… ì‹œì‘ (Task: {task_id})")
    return {"task_id": task_id, "status": "processing"}


@router.get("/tasks/{task_id}")
async def get_task_result(task_id: str):
    """ë°±ê·¸ë¼ìš´ë“œ LLM ì‘ì—… ê²°ê³¼ ì¡°íšŒ (Worker pollingìš©)"""
    redis_key = f"llm_result:{task_id}"
    result_json = redis_client.get(redis_key)

    if not result_json:
        raise HTTPException(status_code=404, detail=f"Task {task_id} not found (may be expired)")

    return json.loads(result_json)
