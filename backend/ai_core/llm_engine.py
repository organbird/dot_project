# =====================================================================
# LLM Engine - ëŒ€í™”í˜• ì–¸ì–´ ëª¨ë¸ ì—”ì§„
# =====================================================================
# ì´ íŒŒì¼ì€ Llama ëª¨ë¸ì„ ì‚¬ìš©í•œ ëŒ€í™” ìƒì„± ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
# - llama-cpp-python ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
# - GPU ê°€ì† ì§€ì›
# - ìŠ¤íŠ¸ë¦¬ë° ë° ì¼ë°˜ ì±„íŒ… ëª¨ë“œ ì§€ì›
# - ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
# - Thread-safe: ë‹¤ì¤‘ ì‚¬ìš©ì í™˜ê²½ì—ì„œ ì•ˆì „í•œ ë™ì‹œì„± ì œì–´
# =====================================================================

import os
import threading
from llama_cpp import Llama

# LLM ë™ì‹œ ì ‘ê·¼ ì œì–´ë¥¼ ìœ„í•œ Lock (ì´ë¯¸ì§€ ìƒì„±/ì±„íŒ… ê°„ ì¶©ëŒ ë°©ì§€)
llm_lock = threading.Lock()

class LLMEngine:
    """
    ëŒ€í™”í˜• ì–¸ì–´ ëª¨ë¸ ì—”ì§„ í´ë˜ìŠ¤

    Llama ëª¨ë¸(GGUF í¬ë§·)ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ìƒì„± ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    GPU ê°€ì†ì„ ì§€ì›í•˜ë©°, ìŠ¤íŠ¸ë¦¬ë° ë° ì¼ë°˜ ëª¨ë“œë¡œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    Attributes:
        model (Llama): llama-cpp-python ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        model_path (str): ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸ ê²½ë¡œ)
    """

    def __init__(self):
        """
        LLMEngine ì´ˆê¸°í™”

        Note:
            - ì´ˆê¸°í™” ì‹œì ì—ëŠ” ëª¨ë¸ì„ ë¡œë“œí•˜ì§€ ì•ŠìŒ
            - load_model()ì„ ëª…ì‹œì ìœ¼ë¡œ í˜¸ì¶œí•´ì•¼ í•¨
        """
        self.model = None
        # Docker ë³¼ë¥¨ì— ë§ˆìš´íŠ¸ëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        self.model_path = "/ai_models/llm/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf"

    def load_model(self):
        """
        ëª¨ë¸ì„ GPU ë©”ëª¨ë¦¬ì— ë¡œë“œ

        ì„œë²„ ì‹œì‘ ì‹œ 1ë²ˆë§Œ í˜¸ì¶œë©ë‹ˆë‹¤. (main.pyì˜ lifespan ì´ë²¤íŠ¸)
        GPU ë ˆì´ì–´ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ì¶”ë¡  ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

        Raises:
            Exception: ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ

        Note:
            - n_gpu_layers=-1: ëª¨ë“  ë ˆì´ì–´ë¥¼ GPUì— ë¡œë“œ
            - n_ctx=8192: ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (í† í° ìˆ˜)
            - verbose=True: ë””ë²„ê¹… ë¡œê·¸ ì¶œë ¥
            - ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ì¬ë¡œë”©í•˜ì§€ ì•ŠìŒ
        """
        if self.model is None:
            print(f"ğŸš€ [LLMEngine] ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_path}")

            # VRAM ì •ë¦¬ (ì´ì „ ëª¨ë¸ ì”ì—¬ë¬¼ ì œê±°)
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    import gc
                    gc.collect()
                    print("âœ… [LLMEngine] VRAM ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ [LLMEngine] VRAM ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

            try:
                self.model = Llama(
                    model_path=self.model_path,
                    n_gpu_layers=-1,  # GPU ë ˆì´ì–´ ì „ì²´ í• ë‹¹ (VRAMì— ëª¨ë‘ ë¡œë“œ)
                    n_ctx=8192,       # ë¬¸ë§¥ ê¸¸ì´ (ê¸¸ê²Œ ì„¤ì •í•˜ë©´ ê¸´ ëŒ€í™” ì²˜ë¦¬ ê°€ëŠ¥)
                    verbose=True      # ë””ë²„ê¹…ìš© ë¡œê·¸ ì¼œê¸°
                )
                print("âœ… [LLMEngine] ëª¨ë¸ ë¡œë”© ì„±ê³µ!")
            except Exception as e:
                print(f"âŒ [LLMEngine] ë¡œë”© ì‹¤íŒ¨: {e}")
                self.model = None  # ëª…ì‹œì ìœ¼ë¡œ None ì„¤ì •
                raise e  # ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ ì„œë²„ ì‹œì‘ì„ ì¤‘ë‹¨í•´ì•¼ í•¨
        else:
            print("âš¡ [LLMEngine] ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

    def unload_model(self):
        """
        ëª¨ë¸ì„ VRAMì—ì„œ ì–¸ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ í•´ì œ

        ì´ë¯¸ì§€ ìƒì„± ë“± ë‹¤ë¥¸ GPU ì‘ì—…ì„ ìœ„í•´ VRAMì„ í™•ë³´í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        if self.model is not None:
            print("ğŸ”„ [LLMEngine] ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
            del self.model
            self.model = None

            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ë° VRAM ìºì‹œ ì •ë¦¬
            try:
                import gc
                gc.collect()  # Python ê°ì²´ ì •ë¦¬

                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    print("âœ… [LLMEngine] VRAM ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ [LLMEngine] VRAM ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

            print("âœ… [LLMEngine] ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
        else:
            print("âš ï¸ [LLMEngine] ì–¸ë¡œë“œí•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

    def is_loaded(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸"""
        return self.model is not None

    def ensure_loaded(self):
        """
        ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ë¡œë“œ (Thread-safe)
        ì´ë¯¸ì§€ ìƒì„± í›„ ì±„íŒ… ì‹œ ìë™ ë³µêµ¬ë¥¼ ìœ„í•´ ì‚¬ìš©
        """
        with llm_lock:
            if self.model is None:
                print("ğŸ”„ [LLMEngine] ëª¨ë¸ ìë™ ë¡œë“œ ì¤‘...")

                # VRAM ì •ë¦¬ ë° ì•ˆì •í™” ëŒ€ê¸°
                try:
                    import gc
                    import time
                    gc.collect()

                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                        print("âœ… [LLMEngine] VRAM ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

                    # llama-cpp-python ë²„ê·¸ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
                    time.sleep(1.0)

                except Exception as e:
                    print(f"âš ï¸ [LLMEngine] VRAM ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

                try:
                    self.load_model()
                except Exception as e:
                    print(f"âš ï¸ [LLMEngine] ìë™ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    print("   ë‹¤ìŒ ìš”ì²­ ì‹œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")

    def chat(self, user_input: str) -> str:
        """
        ì¼ë°˜ ì±„íŒ… ëª¨ë“œ (ì™„ì„±ëœ ì‘ë‹µì„ í•œ ë²ˆì— ë°˜í™˜)

        Args:
            user_input (str): ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë˜ëŠ” ë©”ì‹œì§€

        Returns:
            str: AIì˜ ì™„ì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸

        Note:
            - ë¸”ë¡œí‚¹ ë°©ì‹: ì „ì²´ ì‘ë‹µì´ ìƒì„±ë  ë•Œê¹Œì§€ ëŒ€ê¸°
            - ìŠ¤íŠ¸ë¦¬ë°ì´ í•„ìš” ì—†ëŠ” ê²½ìš° ì‚¬ìš©
            - temperature=0.7: ì ì ˆí•œ ì°½ì˜ì„± (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê²°ì •ì )
        """
        if not self.model:
            return "ì‹œìŠ¤í…œ ì—ëŸ¬: ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        # OpenAI Chat API í˜•ì‹ì˜ ë©”ì‹œì§€ êµ¬ì¡°
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ DOT í”„ë¡œì íŠ¸ì˜ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."},
            {"role": "user", "content": user_input}
        ]

        # ì±„íŒ… ì™„ë£Œ ìƒì„± (ë¸”ë¡œí‚¹)
        response = self.model.create_chat_completion(
            messages=messages,
            max_tokens=1024,     # ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature=0.7,     # ìƒ˜í”Œë§ ì˜¨ë„ (ì°½ì˜ì„± ì¡°ì ˆ)
        )
        return response['choices'][0]['message']['content']

    def chat_stream(self, user_input: str, history: list = None):
        """
        ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ëª¨ë“œ (í† í° ë‹¨ìœ„ë¡œ ì‹¤ì‹œê°„ ìƒì„±)

        ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ë©°,
        ìƒì„±ë˜ëŠ” í† í°ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ yieldí•©ë‹ˆë‹¤.

        Args:
            user_input (str): í˜„ì¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸
            history (list, optional): ì´ì „ ëŒ€í™” ê¸°ë¡
                í˜•ì‹: [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}, ...]

        Yields:
            str: ìƒì„±ëœ í† í° (ë¬¸ì ë˜ëŠ” ë‹¨ì–´ ë‹¨ìœ„)

        Note:
            - ì œë„ˆë ˆì´í„° í•¨ìˆ˜: for ë£¨í”„ë¡œ í† í°ì„ í•˜ë‚˜ì”© ë°›ì•„ì•¼ í•¨
            - historyê°€ ìˆìœ¼ë©´ ë¬¸ë§¥ì„ ì´ì–´ì„œ ë‹µë³€ ìƒì„±
            - temperature=0.7: ì¼ê´€ì„±ê³¼ ì°½ì˜ì„±ì˜ ê· í˜•
            - max_tokens=2048: ê¸´ ë‹µë³€ë„ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •

        Example:
            >>> for token in llm.chat_stream("ì•ˆë…•í•˜ì„¸ìš”", history=[]):
            ...     print(token, end='', flush=True)
        """
        if self.model is None:
            yield "âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            return

        # 1. ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ DOT í”„ë¡œì íŠ¸ì˜ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."}
        ]

        # 2. ì´ì „ ëŒ€í™” ê¸°ë¡(History)ì´ ìˆë‹¤ë©´ ì¤‘ê°„ì— ë¼ì›Œë„£ê¸°
        # (ì›¹ ì„œë²„ê°€ DB ë˜ëŠ” Redisì—ì„œ êº¼ë‚´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•¨)
        if history:
            messages.extend(history)

        # 3. í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ê°€
        messages.append({"role": "user", "content": user_input})

        print(f"ğŸš€ [LLMEngine] ìŠ¤íŠ¸ë¦¬ë° ì¶”ë¡  ì‹œì‘ (ì´ ë©”ì‹œì§€ ìˆ˜: {len(messages)})")

        # 4. ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ(stream=True)ë¡œ í˜¸ì¶œ
        # ë‚´ë¶€ì ìœ¼ë¡œ í† í° ìƒì„± ë£¨í”„ë¥¼ ëŒë©´ì„œ í•˜ë‚˜ì”© yieldí•¨
        stream = self.model.create_chat_completion(
            messages=messages,
            max_tokens=2048,  # ë‹µë³€ ê¸¸ì´ ì œí•œ
            temperature=0.7,
            stream=True       # â˜… í•µì‹¬: ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
        )

        # 5. í•œ í† í°ì”© ê»ì§ˆ ê¹Œì„œ ë°–ìœ¼ë¡œ ë˜ì ¸ì£¼ê¸° (yield)
        for chunk in stream:
            if 'choices' in chunk:
                delta = chunk['choices'][0]['delta']
                if 'content' in delta:
                    yield delta['content']

# =====================================================================
# í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰ ì½”ë“œ
# =====================================================================
# ì‹¤í–‰ë²•: docker compose exec backend python -m ai_core.llm_engine
if __name__ == "__main__":
    bot = LLMEngine()
    bot.load_model()
    print("\nğŸ’¬ í…ŒìŠ¤íŠ¸ ëŒ€í™” ì‹œì‘ (ì¢…ë£Œ: q)")
    while True:
        txt = input("User: ")
        if txt == 'q':
            break
        print(f"Bot: {bot.chat(txt)}")
